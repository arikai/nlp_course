#+AUTHOR: Рогов Я.С.
#+TITLE: Homework 5
#+LANGUAGE: ru
#+LATEX_HEADER: \subject{Автоматическая обработка естественного языка}
#+LATEX_HEADER: \labnum{4}
#+LATEX_HEADER: \variant{}
#+LATEX_HEADER: \professor{Г. Д. Вольгенаннт}
#+LATEX_HEADER: \groupname{P41182}
#+TAGS: noexport

#+STARTUP: showall hideblocks inlineimages indent
#+STARTUP: latexpreview

#+OPTIONS: ':t -:t ::t <:t \n:nil ^:t f:t |:t e:t
#+OPTIONS: author:t broken-links:mark date:t title:t
#+OPTIONS: tex:t toc:nil

#+OPTIONS: H:3

# Do not export TODO-related text, tags, properties,
#+OPTIONS: todo:nil tags:nil prop:nil
# drawers, inline tasks and statistics cookies ([0/3] in TODOs)
#+OPTIONS: d:nil inline:nil stat:nil

#+LATEX_COMPILER: xelatex
#+LATEX_CLASS: itmo-report

#+PROPERTY: header-args :python "python3" :session lab5 :cache yes :exports code :results output :wrap example
* Setup PyTorch
#+begin_src sh :output code
pip3 install pytorch
#+end_src
* Dataset
Brown dataset from NLTK is used with universal tagset
#+begin_src python
import nltk
nltk.download('brown')
nltk.download('universal_tagset')
#+end_src
* Prepare dataset
#+begin_src python
import numpy as np
from nltk.corpus import brown

def prepare_nltk_tagged_sent(sent):
    return tuple(zip(*sent))

sents = np.random.permutation(brown.tagged_sents(tagset='universal'))

TRAINING_SIZE = 1000
TEST_SIZE = 10

training_data = list(map(prepare_nltk_tagged_sent, sents[:TRAINING_SIZE]))

test_data = list(map(prepare_nltk_tagged_sent,
                     sents[TRAINING_SIZE:TRAINING_SIZE+TEST_SIZE]))
#+end_src
* An LSTM Model definition
#+begin_src python
import torch
import torch.nn as nn
import torch.nn.functional as F

class LSTMPoSTagger(nn.Module):
    def __init__(self, embedding_dim, hidden_dim, word2idx, tag2idx):
        super().__init__()

        # Params
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim
        self.vocab_size = len(word2idx)
        self.tagset_size = len(tag2idx)

        # Units
        self.word_embeddings = nn.Embedding(
            num_embeddings = self.vocab_size,
            embedding_dim = self.embedding_dim
        )
        self.lstm = nn.LSTM(
            input_size = self.embedding_dim,
            hidden_size = self.hidden_dim
        )
        self.hidden2tag = nn.Linear(
            in_features = self.hidden_dim,
            out_features = self.tagset_size
        )

    def forward(self, sentence):
        sentlen = len(sentence)

        embeds = self.word_embeddings(sentence).view(sentlen, 1, -1)
        lstm_out, _ = self.lstm(embeds)

        tag_space = self.hidden2tag(lstm_out.view(sentlen, -1))
        tag_scores = F.log_softmax(tag_space, dim=1)
        return tag_scores

class Ent2Idx:
    def __init__(self, e2idict):
        self.e2i = e2idict
        self.i2e = sorted(e2idict, key = lambda x: e2idict[x], reverse=True)

    def __len__(self):
        return len(self.e2i)

    def index(self, item):
        return self.e2i[item]

    def entity(self, idx):
        return self.i2e[idx]

    def indices(self, seq):
        e2i = self.e2i
        return torch.tensor([e2i[x] for x in seq], dtype=torch.long)

    def entities(self, vec):
        i2e = self.i2e
        return list(map(lambda x: i2e[x], vec))


def build_seq2vec_indices(dataset):
    word2idx = dict()
    tag2idx = dict()

  for sent, tags in dataset:
   for word in sent:
     if word not in word2idx:
         word2idx[word] = len(word2idx)
   for tag in tags:
     if tag not in tag2idx:
         tag2idx[tag] = len(tag2idx)
  return (Ent2Idx(word2idx), Ent2Idx(tag2idx))


EMBED_SIZE = 32
HID_DIM = 32
# Use both training and test set to build indices
word2idx, tag2idx = build_seq2vec_indices(training_data + test_data)

model = LSTMPoSTagger(EMBED_SIZE, HID_DIM, word2idx, tag2idx).cuda()
#+end_src
* Training model
#+begin_src python
import torch.optim as optim

loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
EPOCHS = 150

for epoch in range(EPOCHS):
  print(epoch, end=' ')
  for sentence, tags in training_data:
      model.zero_grad()
      tag_scores = model(word2idx.indices(sentence).cuda())
      loss = loss_function(tag_scores, tag2idx.indices(tags).cuda())
      loss.backward()
      optimizer.step()
#+end_src
* Evaluating Results
#+begin_src python
def print_diff(text, expected, got):
  text = ["Text:"] + list(text)
  expected = ["Expected:"] + list(expected)
  got = ["Got:"] + list(got)

  column_sizes = [max(map(len, (t, e, g)))
                  for t, e, g in zip(text, expected, got)]

  def print_row(row):
    for t, sz in zip(row, column_sizes):
      print("{:{}}".format(t, sz), end=' ')
      print()

  for row in (text, expected, got):
    print_row(row)

  total = len(expected)
  correct = sum(map(lambda x: x[0] == x[1], zip(expected, got)))

  print("Correct: {}/{}".format(correct, total))
  print()

with torch.no_grad():
  for sent, tags in test_data:
    raw = model(word2idx.indices(sent).cuda())
    predicted_tags = tag2idx.entities(raw.argmax(1))
    print_diff(sent, tags, predicted_tags)
#+end_src
* Calculating evaluation metric
#+begin_src python
from math import log, inf

with torch.no_grad():
  print("Closer to 1 - better")
  for sent, tags in test_data:
    prediction = model(word2idx.indices(sent).cuda())
    loss = loss_function(prediction, tag2idx.indices(tags).cuda())
    print(loss.neg().exp().item())
#+end_src
